{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90572e29",
   "metadata": {},
   "source": [
    "# ISPA - SearchLight\n",
    "Script adapted from: https://github.com/SylvainTakerkart/inter_subject_pattern_analysis/tree/master/fmri_data\n",
    "\n",
    "Original paper: https://doi.org/10.1016/j.neuroimage.2019.116205"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd30d43",
   "metadata": {},
   "source": [
    "This script demonstrates how to use the searchlight implementation\n",
    "available in nilearn to perform group-level decoding using an\n",
    "inter-subject pattern analysis (ISPA) scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7516cd",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e895617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import re\n",
    "import glob\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from nilearn.image import new_img_like, concat_imgs, clean_img\n",
    "from nilearn.decoding import SearchLight\n",
    "\n",
    "import nibabel as nb\n",
    "from nilearn import plotting\n",
    "\n",
    "import time  # Import the time module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e62fa7",
   "metadata": {},
   "source": [
    "## Opening files to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f8414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the path to your GLM directory\n",
    "data_dir = '/projetos/PRJ1901_AMID/03_PROCS/Postnatal_Affective_Dataset/PROC_DATA/derivatives/GLM'\n",
    "\n",
    "# getting a full list of available subjects\n",
    "files = os.listdir(data_dir)\n",
    "pattern = r'sub-(\\d+)_LSA'\n",
    "subject_numbers = [re.search(pattern, file_name).group(1) for file_name in files if re.search(pattern, file_name)]\n",
    "\n",
    "# List of subjects to include, empty means include all\n",
    "subjects_to_include = []  # Fill with subject identifiers or leave empty to include all\n",
    "\n",
    "beta_flist = []\n",
    "y = []\n",
    "subj_vect = []\n",
    "runs = []\n",
    "        \n",
    "# Create lists from beta map filenames in 'beta_flist', labels in 'y', and subject numbers in 'subj_vect'.\n",
    "for subject_number in subject_numbers:\n",
    "    # If subjects_to_include is empty or subject_number is in the list, process the subject\n",
    "    if not subjects_to_include or subject_number in subjects_to_include:\n",
    "        subject_directory = os.path.join(data_dir, f'sub-{subject_number}_LSA')\n",
    "\n",
    "        # List beta map files for the current subject\n",
    "        beta_map_files = glob.glob(os.path.join(subject_directory, f'sub-{subject_number}_run*_*.nii.gz'))\n",
    "        beta_map_files.sort()\n",
    "        beta_flist.extend(beta_map_files)\n",
    "\n",
    "        # Extract the label from the beta map filename\n",
    "        for beta_map_file in beta_map_files:\n",
    "            label = beta_map_file.split('_')[-3]  # Assumes label is the third last part of the filename\n",
    "            if \"InfOther\" in label:  # Adjust labels to consider Own vs. Other (irrelevant to valence)\n",
    "                label = \"Other\"\n",
    "            elif \"InfOwn\" in label:\n",
    "                label = \"Own\"\n",
    "            y.append(label)\n",
    "            subj_vect.append(subject_number)\n",
    "\n",
    "        # Extract run info from the beta map filename (to be used in standardization)\n",
    "        for beta_map_file in beta_map_files:\n",
    "            run = beta_map_file.split('_')[-4]  # Assumes run is the fourth last part of the filename\n",
    "            runs.append(run)\n",
    "\n",
    "## STANDARDIZATIOIN (per run and subject)\n",
    "\n",
    "# read image data\n",
    "print(\"Reading beta maps from all the subjects...\")\n",
    "\n",
    "fmri_nii_dict = {}\n",
    "\n",
    "for beta_path in beta_flist:\n",
    "    parts = re.split(r'[/_]', beta_path)\n",
    "    subj = parts[-6]\n",
    "    run = 'run-'+parts[-4]\n",
    "    condition = parts[-3]\n",
    "    beta = parts[-2]\n",
    "    \n",
    "    image = nb.load(beta_path)\n",
    "    \n",
    "    # Check if the subject already exists in the dictionary\n",
    "    if subj in fmri_nii_dict:\n",
    "        # If the run exists for the subject, append the data to the existing list\n",
    "        if run in fmri_nii_dict[subj]:\n",
    "            fmri_nii_dict[subj][run].append({'condition': condition, 'beta': beta, 'image': image})\n",
    "        # If the run doesn't exist, create a new entry for the run\n",
    "        else:\n",
    "            fmri_nii_dict[subj][run] = [{'condition': condition,'beta': beta, 'image': image}]\n",
    "    # If the subject doesn't exist, create a new entry for the subject and run\n",
    "    else:\n",
    "        fmri_nii_dict[subj] = {run: [{'condition': condition,'beta': beta, 'image': image}]}\n",
    "\n",
    "# Printing the created dictionary\n",
    "for subj, subj_value in fmri_nii_dict.items():\n",
    "    for run, run_value in subj_value.items():\n",
    "        print(f\"Subject: {subj}, Run: {run}\")\n",
    "        for item in run_value:\n",
    "            print(item)\n",
    "        print('\\n')\n",
    "        \n",
    "## CONCATENATE BETAS IMAGES ##\n",
    "\n",
    "concat_imgs_dict = {}\n",
    "concat_imgs_stand_dict = {}\n",
    "\n",
    "# Concatenating the images for the same run and subject\n",
    "for subj, subj_value in fmri_nii_dict.items():\n",
    "    for run, run_value in subj_value.items():\n",
    "        images_to_concat = [item['image'] for item in run_value]\n",
    "        concatenated_image = concat_imgs(images_to_concat)\n",
    "        print(f\"Subject: {subj}, Run: {run}, Concatenated Beta Images Shape: {concatenated_image.shape}\")\n",
    "        \n",
    "        # Adding concatenated image to the concat_imgs_dict\n",
    "        if subj in concat_imgs_dict:\n",
    "            concat_imgs_dict[subj][run] = concatenated_image\n",
    "        else:\n",
    "            concat_imgs_dict[subj] = {run: concatenated_image}\n",
    "\n",
    "        # Standardized beta maps with clean_img()\n",
    "        stand_image = clean_img(concatenated_image, standardize=True, detrend=False)\n",
    "        print(f\"Subject: {subj}, Run: {run}, Concatenated Standardized Beta Images Shape: {stand_image.shape}\")\n",
    "        \n",
    "        # Adding standardized image to the concat_imgs_stand_dict\n",
    "        if subj in concat_imgs_stand_dict:\n",
    "            concat_imgs_stand_dict[subj][run] = stand_image\n",
    "        else:\n",
    "            concat_imgs_stand_dict[subj] = {run: stand_image}\n",
    "            \n",
    "# Concatenate all subjects #\n",
    "\n",
    "all_images_to_concat = []\n",
    "\n",
    "for subj, subj_value in concat_imgs_stand_dict.items():\n",
    "    for run, stand_image in subj_value.items():\n",
    "        all_images_to_concat.append(stand_image)\n",
    "\n",
    "all_betas_stand = concat_imgs(all_images_to_concat) \n",
    "\n",
    "print(f\"All Betas Stand Shape: {all_betas_stand.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ba189-acd5-444c-8db7-4e9d9718332e",
   "metadata": {},
   "source": [
    "# Adjust Masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdabaa3-8268-4070-a3fc-ae2fad4297da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn.image import math_img, new_img_like, concat_imgs, resample_to_img\n",
    "\n",
    "# reading brain mask\n",
    "mask_brain = nb.load(\"/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/brain_mask.nii.gz\")\n",
    "\n",
    "# Check whether ROI and Betas are in the same space and resample if necessary\n",
    "beta_img = nb.load(beta_map_files[0])\n",
    "if not beta_img.shape == mask_brain.shape:\n",
    "    resampled_mask_brain = resample_to_img(mask_brain, beta_img)\n",
    "print(\"Shape of original Beta image: %s\" % (beta_img.shape,))\n",
    "print(\"Shape of original ROI image: %s\" % (mask_brain.shape,))\n",
    "print(\"Shape of resampled ROI image: %s\" % (resampled_mask_brain.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b35e51-f037-4e7e-8702-6d64af1650fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nilearn.image import math_img, new_img_like, concat_imgs, resample_to_img\n",
    "roi_mask_nii = nb.load(\"/projetos/PRJ1901_AMID/03_PROCS/Postnatal_Affective_Dataset/BIDS_Nilearn/2ndLvPilotSPACE_Binar_NAccBrian_OFC_Septal.nii\")\n",
    "\n",
    "# Check whether ROI and Betas are in the same space and resample if necessary\n",
    "beta_img = nb.load(beta_map_files[0])\n",
    "if not beta_img.shape == roi_mask_nii.shape:\n",
    "    resampled_ROI = resample_to_img(roi_mask_nii, beta_img)\n",
    "print(\"Shape of original Beta image: %s\" % (beta_img.shape,))\n",
    "print(\"Shape of original ROI image: %s\" % (roi_mask_nii.shape,))\n",
    "print(\"Shape of resampled ROI image: %s\" % (resampled_ROI.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa55be",
   "metadata": {},
   "source": [
    "# Setting up for leave-one-subject-out cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb510ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up leave-one-subject-out cross-validation\n",
    "loso = LeaveOneGroupOut()\n",
    "unique_subjects = np.unique(subj_vect)\n",
    "n_splits = loso.get_n_splits(groups=unique_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6ef87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output directory\n",
    "output_dir = \"/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/ISPA_SearchLight_TB_permut\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Calculate the chance level\n",
    "chance_level = 1. / len(np.unique(y))\n",
    "\n",
    "# running searchlight decoding\n",
    "searchlight_radius = 4\n",
    "n_jobs = -1\n",
    "y = np.array(y)\n",
    "single_split_path_list = []\n",
    "print(\"Launching cross-validation...\")\n",
    "for split_ind, (train_inds,test_inds) in enumerate(loso.split(subj_vect,subj_vect,subj_vect)):\n",
    "    print(\"...split {:02d} of {:02d}\".format(split_ind+1, n_splits))\n",
    "    single_split = [(train_inds,test_inds)]\n",
    "    y_train = y[train_inds]\n",
    "    n_samples = len(y_train)\n",
    "    class_labels = np.unique(y_train)\n",
    "    clf = LogisticRegression()\n",
    "    searchlight = SearchLight(resampled_mask_brain,\n",
    "                              #process_mask_img=resampled_ROI,\n",
    "                              radius=searchlight_radius,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=1,\n",
    "                              cv=single_split,\n",
    "                              estimator=clf)\n",
    "    print(\"...mapping the data (this takes a long time) and fitting the model in each sphere\")\n",
    "    searchlight.fit(all_betas_stand, y)\n",
    "\n",
    "    single_split_nii = new_img_like(resampled_mask_brain,searchlight.scores_ - chance_level)\n",
    "    single_split_path = op.join(output_dir, 'ispa_searchlight_accuracy_split{:02d}of{:02d}.nii'.format(split_ind+1,n_splits))\n",
    "    print('Saving score map for cross-validation fold number {:02d}'.format(split_ind+1))\n",
    "    single_split_nii.to_filename(single_split_path)\n",
    "    single_split_path_list.append(single_split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df716a5",
   "metadata": {},
   "source": [
    "# Trying to do Non-parametric Permutations with nilearn\n",
    "Originally it was performed with SnPM MATLAB toolbox to analyse the single-fold (for the inter-subject cross-validation of ISPA) accuracy maps, with 1000 permutations and a significance threshold (p<0,05, FWE corrected).\n",
    "Here, we are trying to do the same, but using nilearn.\n",
    "\n",
    "Original SnPM batch: https://github.com/SylvainTakerkart/inter_subject_pattern_analysis/blob/master/fmri_data/snpm_batch.m\n",
    "\n",
    "## Here we have tried to implement the permutatoin scheme suggested by Stelzer et al. 2013, by permuting labels for X searchlights per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e8df7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output directory\n",
    "output_dir = \"/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/ISPA_SearchLight_TB_permut/withinsubj_permut\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Calculate the chance level\n",
    "chance_level = 1. / len(np.unique(y))\n",
    "\n",
    "# running searchlight decoding\n",
    "searchlight_radius = 4\n",
    "n_jobs = -1\n",
    "y = np.array(y)\n",
    "single_split_path_list = []\n",
    "\n",
    "# Number of permutations within subject\n",
    "n_permutations = 100\n",
    "\n",
    "# List to hold the timings\n",
    "timings = []\n",
    "\n",
    "print(\"Launching cross-validation...\")\n",
    "total_start_time = time.time()  # Capture the start time of the entire process\n",
    "\n",
    "for split_ind, (train_inds,test_inds) in enumerate(loso.split(subj_vect,subj_vect,subj_vect)):\n",
    "    print(\"...split {:02d} of {:02d}\".format(split_ind+1, n_splits))\n",
    "    split_start_time = time.time()  # Start time for this split\n",
    "    \n",
    "    for permutation in range(n_permutations):\n",
    "        print(f\"...within-subject permutation {permutation+1:02d} of {n_permutations}\")\n",
    "        permutation_start_time = time.time()  # Start time for this permutation\n",
    "        \n",
    "        # Append the results to timing list\n",
    "        #timings.append({f\"Start time for split {split_ind+1}': split_start_time})\n",
    "        \n",
    "        # Shuffle y labels for this permutation\n",
    "        np.random.seed(permutation)  # Setting the seed for reproducibility\n",
    "        y_shuffled = np.random.permutation(y)\n",
    "\n",
    "        # Set up the classifier and searchlight with the shuffled labels\n",
    "        clf = LogisticRegression()\n",
    "        searchlight = SearchLight(resampled_mask_brain,\n",
    "                                  radius=searchlight_radius,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  verbose=1,\n",
    "                                  cv=[(train_inds, test_inds)],\n",
    "                                  estimator=clf)\n",
    "        \n",
    "        searchlight_start_time = time.time()  # Start time for this searchlight\n",
    "        # Fit the model with shuffled labels\n",
    "        searchlight.fit(all_betas_stand, y_shuffled)\n",
    "\n",
    "        # Compute accuracy map and subtract chance level\n",
    "        accuracy_map = searchlight.scores_ - chance_level\n",
    "\n",
    "        # Save the accuracy map for this permutation and split\n",
    "        single_split_nii = new_img_like(resampled_mask_brain, accuracy_map)\n",
    "        output_filename = f'split_{split_ind+1:02d}_permutation_{permutation+1:03d}_accuracy.nii'\n",
    "        single_split_path = os.path.join(output_dir, output_filename)\n",
    "        print(f'Saving permutation score map {permutation+1:03d} for split {split_ind+1:02d}')\n",
    "        single_split_nii.to_filename(single_split_path)\n",
    "        \n",
    "        # Print duration for this search light\n",
    "        searchlight_end_time = time.time()\n",
    "    \n",
    "        print(f\"Searchlight {permutation+1} completed in {searchlight_end_time - searchlight_start_time:.2f} seconds.\")\n",
    "        # Append the results to timing list\n",
    "        timings.append({f\"Searchlight {permutation+1}: {searchlight_end_time - searchlight_start_time:.2f}\"})\n",
    "        \n",
    "    # Print duration for this split\n",
    "    split_end_time = time.time()\n",
    "    print(f\"Split {split_ind+1} completed in {split_end_time - split_start_time:.2f} seconds.\")\n",
    "    timings.append({f\"Split {split_ind+1}: {split_end_time - split_start_time:.2f}\"})\n",
    "\n",
    "# Print the total duration\n",
    "total_end_time = time.time()\n",
    "print(f\"Total processing time: {total_end_time - total_start_time:.2f} seconds.\")\n",
    "timings.append({f\"Total processing time: {total_end_time - total_start_time:.2f}\"})\n",
    "\n",
    "new_df = pd.DataFrame(timings)\n",
    "new_df.to_csv('timings_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47e862-1572-4927-b4da-a2a52d16c888",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Randomly select one accuracy map per subject and average (repeat 10n5 x).\n",
    "\n",
    "## Pool of 10n5 chance group accuracy maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d4b862a-9d52-43cf-83b1-7e33a59e5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import glob\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = \"/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/ISPA_SearchLight_TB_permut/withinsubj_permut\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract unique split identifiers based on existing files in directory\n",
    "def get_unique_identifiers(directory, pattern):\n",
    "    files = glob.glob(os.path.join(directory, pattern))\n",
    "    unique_ids = set()\n",
    "    for file in files:\n",
    "        match = re.search(r\"split_(\\d+)_permutation_(\\d+)_accuracy\\.nii\", file)\n",
    "        if match:\n",
    "            unique_ids.add(match.group(1))\n",
    "    return list(unique_ids)\n",
    "\n",
    "# Use the function to get a list of unique split identifiers\n",
    "split_identifiers = get_unique_identifiers(input_dir, \"split_*_permutation_*_accuracy.nii\")\n",
    "\n",
    "# fix identifier for testing\n",
    "#split_identifiers= ['01','02','03']\n",
    "\n",
    "# Preload maps paths for each split identifier\n",
    "split_maps_paths = {}\n",
    "for split_identifier in split_identifiers:\n",
    "    pattern = os.path.join(input_dir, f\"split_{split_identifier}_permutation_*_accuracy.nii\")\n",
    "    split_maps_paths[split_identifier] = glob.glob(pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f41e50e2-098e-4f2f-a2cb-35d877402b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 1541.19 seconds.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iteration_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csvfile, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n\u001b[1;32m     77\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43miteration_results\u001b[49m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Convert the list of selected maps to a string to save in the CSV\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelected Maps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelected Maps\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     81\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iteration_results' is not defined"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import random\n",
    "import csv\n",
    "\n",
    "n_iterations = 100000\n",
    "output_dir = os.path.join(input_dir, \"permu_group_acc_maps\")\n",
    "timings_file = os.path.join(output_dir, \"timings.csv\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load an example NIfTI image to get the shape and affine\n",
    "example_img_path = split_maps_paths[split_identifiers[0]][0]\n",
    "example_img = nib.load(example_img_path)\n",
    "img_shape = example_img.shape\n",
    "affine = example_img.affine\n",
    "\n",
    "def perform_iteration(iteration, output_dir):\n",
    "    start_time = time.time()  # Start timing the iteration\n",
    "    #print(f\"Starting iteration {iteration+1}\")  # Debugging line\n",
    "    sum_of_maps = np.zeros(img_shape)\n",
    "    selected_maps = []  # Initialize a list to store the paths of selected maps\n",
    "    \n",
    "    # Randomly select one map per subject/split and accumulate\n",
    "    for split_id, map_paths in split_maps_paths.items():\n",
    "        selected_map_path = random.choice(map_paths)\n",
    "        selected_maps.append(selected_map_path)  # Store the selected map path\n",
    "        #print(f\"Selected map for {split_id}: {selected_map_path}\")  # Debugging line\n",
    "        map_data = nib.load(selected_map_path).get_fdata()\n",
    "        sum_of_maps += map_data\n",
    "    \n",
    "    averaged_map_data = sum_of_maps / len(split_maps_paths)\n",
    "    averaged_map = nib.Nifti1Image(averaged_map_data, affine)\n",
    "    averaged_map_filename = f\"permuted_group_accuracy_map_{iteration+1:03d}.nii\"\n",
    "    averaged_map_path = os.path.join(output_dir, averaged_map_filename)\n",
    "    averaged_map.to_filename(averaged_map_path)\n",
    "    \n",
    "    # Finish the iteration\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return {\"Iteration\": iteration + 1, \"Duration (s)\": duration}\n",
    "    \n",
    "    #print(f\"Saved permuted group accuracy map for iteration {iteration+1}\")\n",
    "    \n",
    "# Measure the total processing time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Use ProcessPoolExecutor to run iterations in parallel\n",
    "iteration_timings = []\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Start all iterations and return futures\n",
    "    futures = [executor.submit(perform_iteration, iteration, output_dir) for iteration in range(n_iterations)]\n",
    "    \n",
    "    # As each future completes, record its duration\n",
    "    for future in as_completed(futures):\n",
    "        iteration_timings.append(future.result())\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "print(f\"Total processing time: {total_duration:.2f} seconds.\")\n",
    "\n",
    "# Add total processing time to the timings list\n",
    "iteration_timings.append({\"Iteration\": \"Total\", \"Duration (s)\": total_duration, \"Selected Maps\": selected_maps})\n",
    "\n",
    "# Save timings to a CSV file\n",
    "with open(timings_file, 'w', newline='') as file:\n",
    "    fieldnames = [\"Iteration\", \"Duration (s)\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(iteration_timings)\n",
    "    \n",
    "# Save the collected data to a CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"selected_maps.csv\")\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Iteration', 'Duration (s)', 'Selected Maps']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for result in iteration_timings:  # Use iteration_timings here\n",
    "        # Convert the list of selected maps to a string to save in the CSV\n",
    "        result['Selected Maps'] = '|'.join(result['Selected Maps'])\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(f\"Timings saved to {timings_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34771e28",
   "metadata": {},
   "source": [
    "## Need to figure it out how to do steps C and D in the Stelzer paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608d12f-fbbe-4157-b18c-b8872c505da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "input_dir = \"/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/ISPA_SearchLight_TB_permut/withinsubj_permut/permu_group_acc_maps\"\n",
    "\n",
    "# Setup variables\n",
    "n_permutations = 100000\n",
    "voxel_shape = example_img.shape[:3]  # Assuming example_img is defined\n",
    "n_bins = 100\n",
    "histogram_range = (0, 1)  # Assuming accuracies are between 0 and 1\n",
    "\n",
    "# Initialize histograms for each voxel\n",
    "voxel_histograms = np.zeros(voxel_shape + (n_bins,))\n",
    "\n",
    "# Function to update histograms given the accuracies of one permutation\n",
    "def update_histograms(permutation_accuracies, histograms, bins, range):\n",
    "    hist, _ = np.histogram(permutation_accuracies, bins=bins, range=range)\n",
    "    histograms += hist\n",
    "\n",
    "# Assuming you have a function load_permutation_accuracies(index) that loads the accuracies for a given permutation\n",
    "def process_permutation(index):\n",
    "    permutation_accuracies = load_permutation_accuracies(index)  # This function needs to be defined\n",
    "    update_histograms(permutation_accuracies, voxel_histograms, n_bins, histogram_range)\n",
    "\n",
    "# Process each permutation using parallel processing\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    list(executor.map(process_permutation, range(n_permutations)))\n",
    "\n",
    "# Now, voxel_histograms contains the updated histograms for each voxel across all permutations\n",
    "\n",
    "# Calculate the cumulative distribution function (CDF) from histograms\n",
    "voxel_cdfs = np.cumsum(voxel_histograms, axis=3) / n_permutations\n",
    "\n",
    "# Find threshold accuracy where the right-tailed area of the CDF is below 0.001\n",
    "threshold_indices = np.searchsorted(voxel_cdfs, 0.999, side='right')\n",
    "bin_edges = np.linspace(histogram_range[0], histogram_range[1], num=n_bins+1)\n",
    "threshold_accuracies = bin_edges[threshold_indices]\n",
    "\n",
    "# Save threshold map\n",
    "threshold_map = nib.Nifti1Image(threshold_accuracies, affine=example_img.affine)\n",
    "nib.save(threshold_map, os.path.join(output_dir, 'threshold_accuracy_map.nii'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91280f63-7f22-4bd5-90d0-61a68c17adc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e06d8a2",
   "metadata": {},
   "source": [
    "# Generating figures of sinificant brain regions from SnPM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf153918",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = '/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/snpm_batch_cluster_level'\n",
    "fname = '/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/snpm_batch_cluster_level/lP_FDR+.img'\n",
    "\n",
    "#brain_nii = nb.load(result_dir + '/lP_FWE+.img')\n",
    "img = nb.load(fname)\n",
    "nb.save(img, fname.replace('.img', '.nii'))\n",
    "display = plotting.plot_glass_brain(None, display_mode='lyrz')\n",
    "color = 'r'\n",
    "display.add_contours(img, filled=True, levels=[1], colors=color)\n",
    "display.title('regions uncovered by ISPA, cluster-level FWE')\n",
    "#display.savefig(result_dir + '/{}_{}.png'.format(dataset_name, decoding_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dd4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_snpm = '/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/snpm_batch_cluster_level/SnPM_filtered.nii'\n",
    "cluster_snpm = nb.load(cluster_snpm)\n",
    "\n",
    "vmax = -np.log10(1 / 10000)  # ~= -np.log10(1 / n_perm)\n",
    "\n",
    "\n",
    "plotting.plot_stat_map(cluster_snpm,\n",
    "                       cut_coords=(-9, 0, 5, 10),\n",
    "                      title='SnPM FWE cluster mass', vmax=vmax, display_mode='x', cmap = \"Oranges\",\n",
    "                      black_bg=False, draw_cross = False, dim = 1, alpha=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c3dff-5959-4cd8-ad5b-cb0aba232bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster_snpm = '/projetos/PRJ1901_AMID/03_PROCS/TCT_FAPERJ_2023/Postnatal_Affect_Dataset/snpm_batch_cluster_level/SnPM_filtered.nii'\n",
    "cluster_snpm = nb.load(cluster_snpm)\n",
    "\n",
    "vmax = -np.log10(1 / 10000)  # ~= -np.log10(1 / n_perm)\n",
    "\n",
    "\n",
    "plotting.plot_stat_map(cluster_snpm,\n",
    "                       cut_coords=(-20,-15,-9,5),\n",
    "                     vmax=vmax, display_mode='z', cmap = \"Oranges\",\n",
    "                      black_bg=False, draw_cross = False, dim = 1, alpha=0.85)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
